{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from metrics_features import fi_perm\n",
    "from dnn_tau import Dnn_tau\n",
    "from data_extractor import Data_extractor_v2, output_vars_v2\n",
    "import os\n",
    "import fnmatch\n",
    "from utils import normalize, bucketize, split_dataset\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from metrics_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/hnl/prompt_tau/anatuple/nanoV10/TEST9/\"\n",
    "features = deepcopy(output_vars_v2)\n",
    "features.extend(['signal_label', 'channel', 'event_type', 'mass_hyp'])\n",
    "channels = os.listdir(path)\n",
    "relative_path = \"/anatuple/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for i in range(len(features)):\n",
    "    values.append([])\n",
    "data = dict(zip(features, values))\n",
    "\n",
    "for channel in channels:\n",
    "    extractor = Data_extractor_v2(channel)\n",
    "    data = extractor(path+channel+relative_path, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event', 'genWeight', 'deltaphi_12', 'deltaphi_13', 'deltaphi_23', 'deltaeta_12', 'deltaeta_13', 'deltaeta_23', 'deltaR_12', 'deltaR_13', 'deltaR_23', 'pt_123', 'mt_12', 'mt_13', 'mt_23', 'Mt_tot', 'n_tauh', 'signal_label', 'channel', 'event_type', 'mass_hyp']\n",
      "{'tte': 0, 'tee': 1, 'tmm': 2, 'tem': 3, 'ttm': 4}\n"
     ]
    }
   ],
   "source": [
    "N = len(data['event'])\n",
    "n_bkg = N-sum([data['signal_label'][i] for i in range(len(data['signal_label']))])\n",
    "data_norm = normalize(pd.DataFrame(data), 'mass_hyp', n_bkg)\n",
    "data_norm = normalize(data_norm, 'signal_label', n_bkg)\n",
    "data_norm = normalize(data_norm, 'channel', n_bkg)\n",
    "data_processed, channel_indices = bucketize(data_norm, 'channel')\n",
    "print(list(data_processed.keys()))\n",
    "print(channel_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = ['deltaphi_12', 'deltaphi_13', 'deltaphi_23', 'deltaeta_12', 'deltaeta_13', 'deltaeta_23', 'deltaR_12', 'deltaR_13', 'deltaR_23',\n",
    "              'pt_123', 'mt_12', 'mt_13', 'mt_23', 'Mt_tot', 'signal_label', 'channel', 'mass_hyp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events :  6818970\n",
      "Train set : 37.52 %\n",
      "Validation set : 12.51 %\n",
      "Test set : 24.98 %\n",
      "Measurement set : 24.99 %\n"
     ]
    }
   ],
   "source": [
    "train, val, test, meas = split_dataset(data_processed)\n",
    "x_train = train[input_vars]\n",
    "x_test = test[input_vars]\n",
    "x_val = val[input_vars]\n",
    "x_meas = meas[input_vars]\n",
    "\n",
    "label_train = x_train.pop('signal_label').astype(float)\n",
    "label_val = x_val.pop('signal_label').astype(float)\n",
    "label_test = x_test.pop('signal_label').astype(float)\n",
    "label_meas = x_meas.pop('signal_label').astype(float)\n",
    "\n",
    "learning_features = list(x_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 32, 32, 32, 32]\n",
      "Epoch 1/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 1.7969 - accuracy: 0.6520 - val_loss: 0.7797 - val_accuracy: 0.7105\n",
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 1.0454 - accuracy: 0.7124 - val_loss: 0.6765 - val_accuracy: 0.7170\n",
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100000\n",
      "6397/6397 [==============================] - 42s 7ms/step - loss: 1.0069 - accuracy: 0.7262 - val_loss: 0.6908 - val_accuracy: 0.7312\n",
      "Epoch 4/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9830 - accuracy: 0.7347 - val_loss: 0.6897 - val_accuracy: 0.7478\n",
      "Epoch 5/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9690 - accuracy: 0.7340 - val_loss: 0.7103 - val_accuracy: 0.7182\n",
      "Epoch 6/100000\n",
      "6397/6397 [==============================] - 46s 7ms/step - loss: 0.9618 - accuracy: 0.7378 - val_loss: 0.6960 - val_accuracy: 0.7354\n",
      "Epoch 7/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9616 - accuracy: 0.7346 - val_loss: 0.7457 - val_accuracy: 0.7308\n",
      "Epoch 8/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9527 - accuracy: 0.7386 - val_loss: 0.6268 - val_accuracy: 0.7603\n",
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9494 - accuracy: 0.7395 - val_loss: 0.6843 - val_accuracy: 0.7387\n",
      "Epoch 10/100000\n",
      "6397/6397 [==============================] - 42s 7ms/step - loss: 0.9528 - accuracy: 0.7380 - val_loss: 0.7081 - val_accuracy: 0.7268\n",
      "Epoch 11/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9416 - accuracy: 0.7413 - val_loss: 0.6744 - val_accuracy: 0.7381\n",
      "Epoch 12/100000\n",
      "6397/6397 [==============================] - 42s 7ms/step - loss: 0.9351 - accuracy: 0.7445 - val_loss: 0.6708 - val_accuracy: 0.7643\n",
      "Epoch 13/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9348 - accuracy: 0.7430 - val_loss: 0.6280 - val_accuracy: 0.7537\n",
      "Epoch 14/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9313 - accuracy: 0.7466 - val_loss: 0.7300 - val_accuracy: 0.7131\n",
      "Epoch 15/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9305 - accuracy: 0.7442 - val_loss: 0.6555 - val_accuracy: 0.7552\n",
      "Epoch 16/100000\n",
      "6397/6397 [==============================] - 41s 6ms/step - loss: 0.9256 - accuracy: 0.7451 - val_loss: 0.7126 - val_accuracy: 0.7215\n",
      "Epoch 17/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9217 - accuracy: 0.7433 - val_loss: 0.6591 - val_accuracy: 0.7528\n",
      "Epoch 18/100000\n",
      "6397/6397 [==============================] - 42s 7ms/step - loss: 0.9268 - accuracy: 0.7437 - val_loss: 0.6468 - val_accuracy: 0.7559\n",
      "Epoch 19/100000\n",
      "6397/6397 [==============================] - 45s 7ms/step - loss: 0.9262 - accuracy: 0.7454 - val_loss: 0.7011 - val_accuracy: 0.7301\n",
      "Epoch 20/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9278 - accuracy: 0.7441 - val_loss: 0.6315 - val_accuracy: 0.7528\n",
      "Epoch 21/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9244 - accuracy: 0.7502 - val_loss: 0.6469 - val_accuracy: 0.7587\n",
      "Epoch 22/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9187 - accuracy: 0.7477 - val_loss: 0.6859 - val_accuracy: 0.7357\n",
      "Epoch 23/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9167 - accuracy: 0.7464 - val_loss: 0.6317 - val_accuracy: 0.7501\n",
      "Epoch 24/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9121 - accuracy: 0.7490 - val_loss: 0.6656 - val_accuracy: 0.7659\n",
      "Epoch 25/100000\n",
      "6397/6397 [==============================] - 43s 7ms/step - loss: 0.9083 - accuracy: 0.7511 - val_loss: 0.7611 - val_accuracy: 0.7248\n",
      "Epoch 26/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9175 - accuracy: 0.7455 - val_loss: 0.6577 - val_accuracy: 0.7578\n",
      "Epoch 27/100000\n",
      "6397/6397 [==============================] - 45s 7ms/step - loss: 0.9083 - accuracy: 0.7501 - val_loss: 0.6641 - val_accuracy: 0.7568\n",
      "Epoch 28/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9138 - accuracy: 0.7470 - val_loss: 0.6531 - val_accuracy: 0.7634\n",
      "Epoch 29/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9087 - accuracy: 0.7450 - val_loss: 0.6665 - val_accuracy: 0.7336\n",
      "Epoch 30/100000\n",
      "6397/6397 [==============================] - 45s 7ms/step - loss: 0.9079 - accuracy: 0.7501 - val_loss: 0.6493 - val_accuracy: 0.7683\n",
      "Epoch 31/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9076 - accuracy: 0.7473 - val_loss: 0.7124 - val_accuracy: 0.7241\n",
      "Epoch 32/100000\n",
      "6397/6397 [==============================] - 44s 7ms/step - loss: 0.9173 - accuracy: 0.7494 - val_loss: 0.6337 - val_accuracy: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_23_layer_call_fn, dense_23_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, dense_18_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/TEST9_global_v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/TEST9_global_v2/assets\n"
     ]
    }
   ],
   "source": [
    "depths = [len(learning_features)*2]*5\n",
    "print(depths)\n",
    "model = Dnn_tau(list(x_train.keys()), depths=depths)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['loss', 'accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=7)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"./saved_models/checkpoint\",\n",
    "    monitor = \"val_loss\",\n",
    "    save_best_only = True\n",
    ")\n",
    "history = model.fit(x_train, label_train, sample_weight=train['genWeight'], validation_data=(x_val, label_val), epochs=100000, verbose=1, \n",
    "                    batch_size = 400, callbacks=[early_stopping, checkpoint])\n",
    "model = tf.keras.models.load_model('./saved_models/checkpoint')\n",
    "model.save('./saved_models/TEST9_global_v2')\n",
    "# Save history\n",
    "filename = \"./saved_history/TEST9_global_vs.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 07:58:30.822725: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 07:58:31.681928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6673 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2023-05-01 07:58:31.682766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6653 MB memory:  -> device: 1, name: Quadro RTX 4000, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./saved_models/TEST9_global_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaphi_12\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaphi_13\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaphi_23\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaeta_12\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaeta_13\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaeta_23\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaR_12\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaR_13\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  deltaR_23\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  pt_123\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  mt_12\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  mt_13\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  mt_23\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  Mt_tot\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  channel\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "shuffling of  mass_hyp\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "\t column assigned\n",
      "Loss with shuffle evaluated\n",
      "{'deltaphi_12': 0.0, 'deltaphi_13': 0.0, 'deltaphi_23': 0.0, 'deltaeta_12': 0.0, 'deltaeta_13': 0.0, 'deltaeta_23': 0.0, 'deltaR_12': 0.0, 'deltaR_13': 0.0, 'deltaR_23': 0.0, 'pt_123': 0.0, 'mt_12': 0.0, 'mt_13': 0.0, 'mt_23': 0.0, 'Mt_tot': 0.0, 'channel': 0.0, 'mass_hyp': 0.0}\n"
     ]
    }
   ],
   "source": [
    "delta_loss_perm = []\n",
    "for key in learning_features:\n",
    "    loss_no_shuffle, loss_shuffle = fi_perm(model, test, input_vars, key)\n",
    "    delta_loss_perm.append(loss_shuffle-loss_no_shuffle)\n",
    "delta_loss_perm = dict(zip(learning_features, delta_loss_perm))\n",
    "print(delta_loss_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./saved_results/TEST9_global_v1_loss_shuffle.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(delta_loss_perm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./saved_results/TEST9_global_v1_loss_shuffle.pkl\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    delta_loss_perm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test defined\n",
      "y_test defined\n",
      "Loss without shuffle evaluated\n",
      "0     0.619110\n",
      "1     0.510193\n",
      "2     1.376953\n",
      "3     0.476545\n",
      "4     2.452375\n",
      "5     2.357056\n",
      "6     0.287598\n",
      "7     0.482910\n",
      "8     2.813477\n",
      "9     0.016235\n",
      "10    1.205933\n",
      "11    2.167951\n",
      "12    1.520935\n",
      "13    3.052490\n",
      "14    2.583008\n",
      "15    1.210694\n",
      "16    0.617126\n",
      "17    1.739728\n",
      "18    1.042237\n",
      "19    0.391113\n",
      "Name: deltaphi_12, dtype: float64\n",
      "shuffling of  deltaphi_12\n",
      "\t column copied\n",
      "\t column shuffled\n",
      "0     0.619110\n",
      "1     0.510193\n",
      "2     1.376953\n",
      "3     0.476545\n",
      "4     2.452375\n",
      "5     2.357056\n",
      "6     0.287598\n",
      "7     0.482910\n",
      "8     2.813477\n",
      "9     0.016235\n",
      "10    1.205933\n",
      "11    2.167951\n",
      "12    1.520935\n",
      "13    3.052490\n",
      "14    2.583008\n",
      "15    1.210694\n",
      "16    0.617126\n",
      "17    1.739728\n",
      "18    1.042237\n",
      "19    0.391113\n",
      "Name: deltaphi_12, dtype: float64\n",
      "\t column assigned\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/glardon/TP4b/feature_importance_analysis.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blphepc119.epfl.ch/home/glardon/TP4b/feature_importance_analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(x_test[key][:\u001b[39m20\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blphepc119.epfl.ch/home/glardon/TP4b/feature_importance_analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m column assigned\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blphepc119.epfl.ch/home/glardon/TP4b/feature_importance_analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss_shuffle, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(x_test, y_test, sample_weight\u001b[39m=\u001b[39;49mtest[\u001b[39m'\u001b[39;49m\u001b[39mgenWeight\u001b[39;49m\u001b[39m'\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blphepc119.epfl.ch/home/glardon/TP4b/feature_importance_analysis.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss with shuffle evaluated\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/training.py:1681\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m   data_handler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler\n\u001b[1;32m   1679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1680\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1681\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   1682\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   1683\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   1684\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1685\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1686\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   1687\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   1688\u001b[0m       epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1689\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1690\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1691\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1692\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1693\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[1;32m   1695\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1398\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1399\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1148\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1150\u001b[0m     x,\n\u001b[1;32m   1151\u001b[0m     y,\n\u001b[1;32m   1152\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1153\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1154\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1155\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1156\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1157\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1158\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1159\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1160\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1161\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m   1163\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/data_adapter.py:241\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m sample_weight_modes \u001b[39m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    238\u001b[0m     sample_weights, sample_weight_modes)\n\u001b[1;32m    240\u001b[0m \u001b[39m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m (sample_weights, _, _) \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39;49mhandle_partial_sample_weights(\n\u001b[1;32m    242\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    244\u001b[0m inputs \u001b[39m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[1;32m    246\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mint\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs))\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/training_utils.py:76\u001b[0m, in \u001b[0;36mhandle_partial_sample_weights\u001b[0;34m(outputs, sample_weights, sample_weight_modes, check_all_flat)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Adds 1.0 as sample weights for the outputs for which there is no weight.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m  describing the raw sample weights.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m any_sample_weight \u001b[39m=\u001b[39m sample_weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m     75\u001b[0m     w \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m sample_weights)\n\u001b[0;32m---> 76\u001b[0m partial_sample_weight \u001b[39m=\u001b[39m any_sample_weight \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39;49m(\n\u001b[1;32m     77\u001b[0m     w \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m sample_weights)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m any_sample_weight:\n\u001b[1;32m     80\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, any_sample_weight, partial_sample_weight\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/keras/engine/training_utils.py:76\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Adds 1.0 as sample weights for the outputs for which there is no weight.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m  describing the raw sample weights.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m any_sample_weight \u001b[39m=\u001b[39m sample_weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m     75\u001b[0m     w \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m sample_weights)\n\u001b[0;32m---> 76\u001b[0m partial_sample_weight \u001b[39m=\u001b[39m any_sample_weight \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m     77\u001b[0m     w \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m sample_weights)\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m any_sample_weight:\n\u001b[1;32m     80\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, any_sample_weight, partial_sample_weight\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7298\u001b[0m, in \u001b[0;36m_TensorIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limit:\n\u001b[1;32m   7297\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m-> 7298\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tensor[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index]\n\u001b[1;32m   7299\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   7300\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1039\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\n\u001b[1;32m   1034\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1035\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstrided_slice\u001b[39m\u001b[39m\"\u001b[39m, [tensor] \u001b[39m+\u001b[39m begin \u001b[39m+\u001b[39m end \u001b[39m+\u001b[39m strides,\n\u001b[1;32m   1036\u001b[0m     skip_on_eager\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m   1037\u001b[0m   \u001b[39mif\u001b[39;00m begin:\n\u001b[1;32m   1038\u001b[0m     packed_begin, packed_end, packed_strides \u001b[39m=\u001b[39m (stack(begin), stack(end),\n\u001b[0;32m-> 1039\u001b[0m                                                 stack(strides))\n\u001b[1;32m   1040\u001b[0m     \u001b[39mif\u001b[39;00m (packed_begin\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m         packed_end\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64 \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m         packed_strides\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mint64):\n\u001b[1;32m   1043\u001b[0m       \u001b[39mif\u001b[39;00m packed_begin\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtypes\u001b[39m.\u001b[39mint64:\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1424\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1422\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1423\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> 1424\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1425\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1426\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1692\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[1;32m   1694\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1695\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1697\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1698\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/tau-ml/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_test = test[input_vars]\n",
    "print(\"x_test defined\")\n",
    "y_test = x_test.pop('signal_label').astype(float)\n",
    "print(\"y_test defined\")\n",
    "\n",
    "loss_no_shuffle, _ = model.evaluate(x_test, y_test, sample_weight=test['genWeight'], verbose=0)\n",
    "print(\"Loss without shuffle evaluated\")\n",
    "\n",
    "key = input_vars[0]\n",
    "print(x_test[key][:20])\n",
    "x_test[key] = np.random.permutation(x_test[key])\n",
    "print(x_test[key][:20])\n",
    "\n",
    "print(\"\\t column assigned\")\n",
    "loss_shuffle, _ = model.evaluate(x_test, y_test, sample_weight=test['genWeight'], verbose=0)\n",
    "print(\"Loss with shuffle evaluated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
